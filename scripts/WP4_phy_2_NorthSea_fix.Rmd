---
title: "WP4_phy_NorthSea_fix.Rmd"
author: "Luuk van der Heijden"
date: "15 june 2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Global options

```{r global_options, include=FALSE}

knitr::opts_chunk$set(fig.width=12, fig.height = 10, fig.path = 'Figs/',
                      echo = FALSE, warning = FALSE, message = FALSE)

require(robis)
require(lubridate)
require(tidyverse)

```

# Here we load the data that has been extracted from the OBIS server. 

In the next step of the analysis, we filter out all datasets that have species known to be phytoplankton, thus excluding the seaweeds etc from the phylum that we chose in the previous section. We make a list of these data sets, check for the largest which with we start. Then we look up their metadata and store this information in a .csv file. We determine their number of unique phytoplankton species.

In a manual selection step, we fill a field called 'include' to determine what datasets we are going to use in subsequent analysis. We re-read the .csv file and use it to collect the final set of records to be used. We store these records in a binary file.


```{r manipulating of datasets, warning = FALSE}

# #############################################################################
# Load these records with data
# #############################################################################
all2Data <- read_delim(file.path("../data/derived_data/all2Data.csv"), delim = ";")

# Use phyla that are associated with phytoplankton
#non_phy_data <- all2Data %>%
#  filter(!phylum %in% c("Cyanobacteria", "Bigyra", "Cercozoa", "Ciliophora", "Cryptophyta", "Foraminifera", "Haptophyta", "Heliozoa", "Myzozoa",
#                        "Ochrophyta", "Oomycota", "Radiozoa", "Chlorophyta", "Choanozoa", "Euglenozoa", NA))

# Distinct species in phy_data
#non_phy_spec <- non_phy_data %>%
#  distinct(scientificnameaccepted, .keep_all = TRUE) %>%
#  select(scientificnameaccepted, phylum, scientificnameauthorship)

#write.csv(non_phy_spec, file.path("../data/derived_data/non_phy_spec.csv"), row.names = FALSE)

# Use phyla that are associated with phytoplankton and that don't go up to genus / species level
phy_data <- all2Data %>%
  filter(phylum %in% c("Cyanobacteria", "Bigyra", "Cercozoa", "Ciliophora", "Cryptophyta", "Foraminifera", "Haptophyta", "Heliozoa", "Myzozoa",
                       "Ochrophyta", "Oomycota", "Radiozoa", "Chlorophyta", "Choanozoa", "Euglenozoa", NA)) %>%
  filter(!genus %in% NA)  

# Distinct species in phy_data
phy_spec <- phy_data %>%
  distinct(scientificnameaccepted, .keep_all = TRUE) %>%
  select(scientificnameaccepted, aphiaidaccepted, phylum, scientificnameauthorship) 
  
# Write csv
write.csv(phy_spec, file.path("../data/derived_data/phy_spec.csv"), row.names = FALSE)


# Exclude the species that are not phytoplankton, they are summed in this list. This is specific
phy_corrected <- read_delim(file.path("../data/derived_data/phy_spec_corrected.csv"), delim = ";")

# Select phytoplankton only in this file
phy_spec_used <- phy_corrected %>%
  filter(phy_or_not %in% "phy")

# #############################################################################
# Here we do the first cleaning of the datasets
# 1. We exclude non-phytoplankton species that have been selected manually for your dataset (see file uploaded above)
# 2. We exclude the observation if it doesn't contain any data on genus or species level
# 3. We exclude the duplicates for observations with the same aphiaID, dataset, date and location
# #############################################################################
clean_phy <- phy_data %>% 
  filter(scientificnameaccepted %in% phy_spec_used$scientificnameaccepted) %>%    # Exclude non-phytoplankton
  distinct(aphiaid, datasetID, datecollected, decimallongitude, decimallatitude, .keep_all = TRUE)# Filter out the duplicates


```

## Here we do some manual repairs on datasets that are still not usable. 

```{r fix a number of datasets}

# #############################################################################
# Here we do some manual repairs on datasets that are still not usable, we fix:
# Fill in missing eventID's with date and location 
# Extract the aphiaid from aphiaidaccepted
# Duplicates that occurred due to two sample moments on one day
# #############################################################################
fix_phy <- clean_phy %>%
  distinct(aphiaidaccepted, datasetid, datecollected, decimallatitude, decimallongitude, .keep_all = TRUE) %>%       # Select distinct objects
  mutate(month = lubridate::month(datecollected)) %>%                                                                # Create column with months
  mutate(year = lubridate::year(datecollected)) %>%                                                                  # Create column with years
  mutate(datasetID = as.numeric(datasetID)) %>%                                                                      # Make numeric
  unite(eventID_2, c("datecollected", "decimallatitude", "decimallongitude"), sep = "-", remove = FALSE) %>%         # Make eventID were not present
  mutate(eventIDnew = ifelse(is.na(eventid), eventID_2, eventid)) %>%
  mutate(aphiaid_new = str_extract(aphiaidaccepted, "[^http://marinespecies.org/aphia.php?p=taxdetails&id=]+$")) %>% # Remove website from aphiaid
  select(-eventid, -eventID_2, -coordinateuncertaintyinmeters, -aphiaidaccepted, -aphiaid) %>%       # Remove columns not necessary
  rename(eventid = eventIDnew, aphiaid = aphiaid_new)                                                                # Rename these columns created

# Save file
save(fix_phy, file = "../data/raw_data/fix_phy.Rdata")

  
```

## Reproducibility

```{r reproducibility}
# Date time
Sys.time()

# Here we store the session info for this script
sessioninfo::session_info()

# repository
git2r::repository()

```

